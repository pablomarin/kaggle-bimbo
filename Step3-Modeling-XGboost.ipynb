{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Building in XGBoost\n",
    "\n",
    "This is a great article for tunning XGboost: http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "windows=False\n",
    "if (windows):\n",
    "    mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "    os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import boto # to download from AWS S3 buckets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "import math\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's define variables that will define the behaviour of the whole script\n",
    "s3_path = 'http://bbts-kaggle.s3.amazonaws.com/bimbo/Pablo/'\n",
    "use_validation=True # splits train data into train + val sets\n",
    "val_week_threshold = 8 # (possible values 8 or 9)  - weeks 3,4,5,6,7 are train, and week 8.9 are val\n",
    "trimmed = True # removes weeks which doesn't have all the lags. If False, fills empty lags with 0\n",
    "lag = 5  # shifted mean_demand up to \"lag\" weeks\n",
    "if (val_week_threshold == 8): lag = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading File: train_modified_holdout8_trimmed.csv  ...\n",
      "Downloading File: val_modified_holdout8_trimmed.csv  ...\n",
      "Downloading File: test_modified_holdout8_trimmed.csv  ...\n",
      "Time passed: 0hour:4min:26sec\n"
     ]
    }
   ],
   "source": [
    "#now we load our modified train and test set\n",
    "tic()\n",
    "sufix=\"\"\n",
    "if (use_validation): \n",
    "    sufix += \"_holdout\"\n",
    "    sufix += repr(val_week_threshold)\n",
    "if (trimmed): sufix += \"_trimmed\"\n",
    "\n",
    "print ('Downloading File: train_modified{}.csv  ...'.format(sufix))\n",
    "train = pd.read_csv(\"{}train_modified{}.csv\".format(s3_path,sufix),\n",
    "                    dtype = {'Canal_ID': 'int8',\n",
    "                            'brand': 'int8',\n",
    "                            'prodtype_cluster': 'int32',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int32',\n",
    "                            'ZipCode': 'uint32',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int32',\n",
    "                            'Producto_ID_clust_ID':'int32',\n",
    "                            'Ruta_SAK_clust_ID':'int32',\n",
    "                            'Agencia_ID_clust_ID':'int32',\n",
    "                            'Cliente_ID_clust_ID':'int32'},\n",
    "                   )\n",
    "\n",
    "if (use_validation):\n",
    "    print ('Downloading File: val_modified{}.csv  ...'.format(sufix))\n",
    "    val = pd.read_csv(\"{}val_modified{}.csv\".format(s3_path,sufix),\n",
    "                    dtype = {'Canal_ID': 'int8',\n",
    "                            'brand': 'int8',\n",
    "                            'prodtype_cluster': 'int32',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int32',\n",
    "                            'ZipCode': 'uint32',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int32',\n",
    "                            'Producto_ID_clust_ID':'int32',\n",
    "                            'Ruta_SAK_clust_ID':'int32',\n",
    "                            'Agencia_ID_clust_ID':'int32',\n",
    "                            'Cliente_ID_clust_ID':'int32'},\n",
    "                   ) \n",
    "\n",
    "print ('Downloading File: test_modified{}.csv  ...'.format(sufix))\n",
    "test = pd.read_csv(\"{}test_modified{}.csv\".format(s3_path,sufix),\n",
    "                    dtype = {'id': 'uint32',\n",
    "                            'Canal_ID': 'int8',\n",
    "                            'brand': 'int8',\n",
    "                            'prodtype_cluster': 'int32',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int32',\n",
    "                            'ZipCode': 'uint32',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int32',\n",
    "                            'Producto_ID_clust_ID':'int32',\n",
    "                            'Ruta_SAK_clust_ID':'int32',\n",
    "                            'Agencia_ID_clust_ID':'int32',\n",
    "                            'Cliente_ID_clust_ID':'int32'},\n",
    "                      )\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define target and ID columns:\n",
    "target = 'log_target'\n",
    "IDcol = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Semana</th>\n",
       "      <th>Agencia_ID</th>\n",
       "      <th>Canal_ID</th>\n",
       "      <th>Ruta_SAK</th>\n",
       "      <th>Cliente_ID</th>\n",
       "      <th>Producto_ID</th>\n",
       "      <th>log_target</th>\n",
       "      <th>pairs_mean</th>\n",
       "      <th>Log_Target_mean_lag1</th>\n",
       "      <th>Log_Target_mean_lag2</th>\n",
       "      <th>...</th>\n",
       "      <th>brand</th>\n",
       "      <th>prodtype_cluster</th>\n",
       "      <th>Qty_Ruta_SAK_Bin</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>week_ct</th>\n",
       "      <th>NombreCliente</th>\n",
       "      <th>Producto_ID_clust_ID</th>\n",
       "      <th>Ruta_SAK_clust_ID</th>\n",
       "      <th>Agencia_ID_clust_ID</th>\n",
       "      <th>Cliente_ID_clust_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1216</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.138243</td>\n",
       "      <td>0.128988</td>\n",
       "      <td>0.081382</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1238</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.138243</td>\n",
       "      <td>0.128988</td>\n",
       "      <td>0.128988</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1238</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.138243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128988</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1240</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.230405</td>\n",
       "      <td>0.257975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1240</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.230405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257975</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Semana  Agencia_ID  Canal_ID  Ruta_SAK  Cliente_ID  Producto_ID  \\\n",
       "0       8        1110         7      3301       15766         1216   \n",
       "1       8        1110         7      3301       15766         1238   \n",
       "2       9        1110         7      3301       15766         1238   \n",
       "3       8        1110         7      3301       15766         1240   \n",
       "4       9        1110         7      3301       15766         1240   \n",
       "\n",
       "   log_target  pairs_mean  Log_Target_mean_lag1  Log_Target_mean_lag2  \\\n",
       "0    1.791759    0.138243              0.128988              0.081382   \n",
       "1    1.386294    0.138243              0.128988              0.128988   \n",
       "2    1.098612    0.138243              0.000000              0.128988   \n",
       "3    1.098612    0.230405              0.257975              0.000000   \n",
       "4    1.098612    0.230405              0.000000              0.257975   \n",
       "\n",
       "          ...           brand  prodtype_cluster  Qty_Ruta_SAK_Bin  ZipCode  \\\n",
       "0         ...               1                 2                 1     2008   \n",
       "1         ...               1                 2                 1     2008   \n",
       "2         ...               1                 2                 1     2008   \n",
       "3         ...               1                14                 1     2008   \n",
       "4         ...               1                14                 1     2008   \n",
       "\n",
       "   week_ct  NombreCliente  Producto_ID_clust_ID  Ruta_SAK_clust_ID  \\\n",
       "0        0             11                    20                 68   \n",
       "1        0             11                    20                 68   \n",
       "2        1             11                    20                 68   \n",
       "3        0             11                    24                 68   \n",
       "4        1             11                    24                 68   \n",
       "\n",
       "   Agencia_ID_clust_ID  Cliente_ID_clust_ID  \n",
       "0                    5                  100  \n",
       "1                    5                  100  \n",
       "2                    5                  100  \n",
       "3                    5                  100  \n",
       "4                    5                  100  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.pop('predictions')\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple models per client cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we said on our prior step (Models wiht scikit-learn) that we need to deal with the data set high variance. Let's do this first:\n",
    "\n",
    "Looking at the plot below, created on the clustering-by-demand on the feature engineering notebook, we see that some client cluster's demand behave very differntly from others. So this explain why our model is failing on predicting accurately for all of them.\n",
    "We are going then to create a wrapper function to create as many models as Client Clusters by demand are (Cliente_ID_clust_ID). Let's see if the scores are bettter individually, and if the concatenation of all 300 models yields a better overall RSMLE than our baseline 0.47.\n",
    "\n",
    "In order to do this, we are going to create a wrapper function called clusters_fit , who is going to iterate over all cluster and call model_fit in all of them. At the end it concatenates de results.\n",
    "\n",
    "![Image of Variables vs Hypothesis](./input-data/h2o-clustByDem_Cliente_ID_400.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def model_fit(alg, ctrain, cval, ctest, predictors, target, IDcol):\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    watchlist = [(cval[predictors], cval[target])]\n",
    "    alg.fit(ctrain[predictors], ctrain[target], eval_set=watchlist, eval_metric='rmse', early_stopping_rounds=20, verbose=False)\n",
    "\n",
    "\n",
    "    #Predict training set:\n",
    "    ctrain[\"predictions\"] = alg.predict(ctrain[predictors])\n",
    "    ctrain[\"predictions\"] = np.maximum(ctrain[\"predictions\"], 0)\n",
    "\n",
    "    \n",
    "    #Predict validation (holdout) set:\n",
    "    cval[\"predictions\"] = alg.predict(cval[predictors])\n",
    "    cval[\"predictions\"] = np.maximum(cval[\"predictions\"], 0)# we make all negative numbers = 0 since there cannot be a negative demand\n",
    "\n",
    "    \n",
    "    #Predict on testing data: we need to revert it back to target by applying expm1\n",
    "    ctest[target] = alg.predict(ctest[predictors])\n",
    "    ctest[target] = np.maximum(ctest[target], 0) # we make all negative numbers = 0 since there cannot be a negative demand\n",
    "    \n",
    "    return ctrain[[target,\"predictions\"]], cval[[target,\"predictions\"]], ctest[[IDcol,target]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import sys\n",
    "def clusters_fit (alg, dtrain, dval, dtest, predictors, target, IDcol):\n",
    "    \n",
    "    train_predictions = pd.DataFrame(index=[target,\"predictions\"])\n",
    "    val_predictions = pd.DataFrame(index=[target,\"predictions\"])\n",
    "    test_predictions = pd.DataFrame(index=[IDcol,target])\n",
    "    \n",
    "    clusters_list = train.Cliente_ID_clust_ID.drop_duplicates().get_values()\n",
    "    np.random.shuffle(clusters_list)\n",
    "    \n",
    "    lcluster = 0;\n",
    "    \n",
    "    for cluster in tqdm(clusters_list):  #tqdm is a genius progress bar library to print out progress\n",
    "        \n",
    "        #we get the cluster train,val, test data\n",
    "\n",
    "        ctrain = dtrain.loc[dtrain[\"Cliente_ID_clust_ID\"] == cluster]\n",
    "        cval   = dval.loc[dval[\"Cliente_ID_clust_ID\"] == cluster]\n",
    "        ctest  = dtest.loc[dtest[\"Cliente_ID_clust_ID\"] == cluster]\n",
    "        \n",
    "        sys.stdout.write('\\rCluster: {:.0f}'.format(cluster))\n",
    "        \n",
    "        #we train the cluster\n",
    "        ctrain, cval, ctest = model_fit(model, ctrain, cval, ctest, predictors, target, IDcol)\n",
    "        \n",
    "        #rsmle_train =  np.sqrt(metrics.mean_squared_error(ctrain[target], ctrain[\"predictions\"]))\n",
    "        #rsmle_val = np.sqrt(metrics.mean_squared_error(cval[target], cval[\"predictions\"]))\n",
    "            \n",
    "        #concatenate each cluster result\n",
    "        train_predictions = pd.concat([train_predictions,ctrain],ignore_index=True)\n",
    "        val_predictions = pd.concat([val_predictions,cval],ignore_index=True)\n",
    "        test_predictions = pd.concat([test_predictions,ctest],ignore_index=True)\n",
    "        \n",
    "        #train_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "        #val_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "        #test_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "          \n",
    "        #acc_rsmle_train =  np.sqrt(metrics.mean_squared_error(train_predictions[target], train_predictions[\"predictions\"]))\n",
    "        #acc_rsmle_val = np.sqrt(metrics.mean_squared_error(val_predictions[target], val_predictions[\"predictions\"]))\n",
    "        #rows_pct = cval.shape[0]*100/dval.shape[0]\n",
    "\n",
    "        #sys.stdout.write('\\r')\n",
    "        #sys.stdout.write('\\tRMSLE T: {:.4f}\\tRMSLE V: {:.4f}\\tRowsPct: {:.4f}\\tACC. RSMLE TRAIN: {:.4f}\\tACC. RSMLE VAL: {:.4f}'.format(\n",
    "        #       rsmle_train, rsmle_val ,rows_pct, acc_rsmle_train, acc_rsmle_val))\n",
    "        #sys.stdout.flush()\n",
    "\n",
    "        \n",
    "    #For some reason this function is adding to NaN rows at the beggining, I don't know why, but we'll remove them\n",
    "    train_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "    val_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "    test_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "    \n",
    "    return train_predictions, val_predictions, test_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_submit(dtrain, dval, dtest, filename):\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print ('RMSLE TRAIN: ', np.sqrt(metrics.mean_squared_error(dtrain[target], dtrain[\"predictions\"])))\n",
    "    print ('RMSLE VAL: ', np.sqrt(metrics.mean_squared_error(dval[target], dval[\"predictions\"])))\n",
    "    \n",
    "    #Predict on testing data: we need to revert it back to target by applying expm1\n",
    "    dtest[target] = np.expm1(dtest[target])\n",
    "    dtest[target] = np.maximum(dtest[target], 0) # we make all negative numbers = 0 since there cannot be a negative demand\n",
    "  \n",
    "    \n",
    "    print ('NUM ROWS PREDICTED: ', dtest.shape[0] )\n",
    "    print ('NUM NEGATIVES PREDICTED: ', dtest[target][dtest[target] < 0].count())\n",
    "    print ('MIN TARGET PREDICTED: ', dtest[target].min())\n",
    "    print ('MEAN TARGET PREDICTED: ', dtest[target].mean())\n",
    "    print ('MAX TARGET PREDICTED: ', dtest[target].max())\n",
    "    \n",
    "    #Export submission file:\n",
    "    submission = dtest.copy()\n",
    "    submission[IDcol] = submission[IDcol].astype(int)\n",
    "    submission.rename(columns={target: 'Demanda_uni_equil'}, inplace=True)\n",
    "    submission.to_csv(\"./Submissions/\"+filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case there is no validation, we make val = train\n",
    "if not (use_validation):\n",
    "    val = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg6 - XGB - Train each client cluster separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training each of the client clusters separately and see if we have good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['Canal_ID', 'Log_Target_mean_lag1', 'Log_Target_mean_lag2', 'Log_Target_mean_lag3', 'Log_Target_mean_lag4', \n",
    "              'Lags_sum', 'brand', 'prodtype_cluster', 'Qty_Ruta_SAK_Bin', 'ZipCode', 'Producto_ID_clust_ID']\n",
    "\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators = 400, objective=\"reg:linear\", learning_rate= 0.1, max_depth=5,\n",
    "                         subsample=0.85,colsample_bytree=0.8, min_child_weight = 1, gamma = 0.1, scale_pos_weight = 1)\n",
    "\n",
    "tic()\n",
    "dt, dv, dte = clusters_fit(model, train, val, test, predictors, target, IDcol)\n",
    "report_submit(dt, dv, dte, 'alg6_{}.csv'.format(sufix))\n",
    "tac()\n",
    "\n",
    "#Plot Histogram of target and prediction distributions\n",
    "plt.hist(dv['log_target'], 100, alpha=0.5, label='target')\n",
    "plt.hist(dv['predictions'], 100, alpha=0.5, label='predictions') \n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg7 - XGB - Train only one batch\n",
    "\n",
    "And now let's compare it with training the complete training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [x for x in train.columns if x not in [target]+[IDcol]]\n",
    "predictors.remove('pairs_mean')\n",
    "\n",
    "#model = xgb.XGBRegressor(n_estimators = 300, objective=\"reg:linear\", learning_rate= 0.1, max_depth=10,\n",
    "#                         subsample=0.85,colsample_bytree=0.7)\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators = 500, objective=\"reg:linear\", learning_rate= 0.1, max_depth=5,\n",
    "                         subsample=0.85,colsample_bytree=0.8, min_child_weight = 1, gamma = 0.1, scale_pos_weight = 1)\n",
    "\n",
    "tic()\n",
    "dt, dv, dte = model_fit(model, train, val, test, predictors, target, IDcol)\n",
    "report_submit(dt, dv, dte, 'alg7_{}.csv'.format(sufix))\n",
    "tac()\n",
    "\n",
    "feat_imp = pd.Series(model.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "\n",
    "#Plot Histogram of target and prediction distributions\n",
    "plt.hist(val['log_target'], 100, alpha=0.5, label='target')\n",
    "plt.hist(val['predictions'], 100, alpha=0.5, label='predictions') \n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#Plot Training and Validation scoring\n",
    "plt.plot(model.evals_result()['validation_0']['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
